{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "178a5f14-fcaa-47ab-a800-0f820c126af1",
   "metadata": {},
   "source": [
    "# ## Question 1------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d7701e-cf98-4cae-8f0b-9950c745d2a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "The main difference between the Euclidean distance metric and the Manhattan (or L1 norm) distance metric lies in how they measure the \n",
    "distance between two points in a multidimensional space:\n",
    "\n",
    "Euclidean Distance:\n",
    "\n",
    "Also known as L2 norm or straight-line distance.\n",
    "Represents the length of the shortest path between two points in a Euclidean space.\n",
    "Computed as the square root of the sum of squared differences between corresponding elements of the two points.\n",
    "More sensitive to differences along all dimensions.\n",
    "Manhattan Distance:\n",
    "\n",
    "Also known as L1 norm or city block distance.\n",
    "Represents the sum of the absolute differences between corresponding elements of the two points.\n",
    "The distance is calculated by moving horizontally and vertically, similar to navigating city blocks.\n",
    "Less sensitive to differences along individual dimensions.\n",
    "How this Difference Affects KNN Performance:\n",
    "Sensitivity to Dimensionality:\n",
    "\n",
    "Euclidean distance is sensitive to differences along all dimensions, while Manhattan distance is less sensitive. In high-dimensional spaces,\n",
    "Euclidean distance may be influenced by irrelevant dimensions, potentially affecting the performance of KNN.\n",
    "Dominance of Large-Scale Features:\n",
    "\n",
    "Euclidean distance can be influenced more by features with larger scales, as it considers the squared differences. In contrast, Manhattan \n",
    "distance is less affected by the scale of individual features, making it more robust when features have different scales.\n",
    "Data Characteristics:\n",
    "\n",
    "The choice between Euclidean and Manhattan distance depends on the characteristics of the data. If the data is distributed in a grid-like \n",
    "fashion, Manhattan distance might be more suitable. On the other hand, if the data is spread out in a more isotropic manner, Euclidean\n",
    "distance may perform well.\n",
    "Performance in Specific Scenarios:\n",
    "\n",
    "In some scenarios, one distance metric may outperform the other based on the nature of the data and the problem at hand. It is often \n",
    "beneficial to experiment with both metrics and choose the one that performs better in cross-validation or grid search.\n",
    "In summary, the choice between Euclidean and Manhattan distance in KNN should be made based on the characteristics of the data, the\n",
    "dimensionality of the feature space, and the potential impact of individual feature scales on the distance calculations. Experimentation \n",
    "and careful consideration of the problem context are essential for choosing the most suitable distance metric for a specific KNN application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031e3bf2-ad71-4b45-9879-057bedaff81d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88524de-23ac-4caa-8475-23a389730d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f16f4011-2dd6-42a4-a05e-4a1bac19425e",
   "metadata": {},
   "source": [
    "## Qestion 2 --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db6999d-bea5-49fb-bd82-931a82c42292",
   "metadata": {},
   "outputs": [],
   "source": [
    "Choosing the optimal value for the hyperparameter K in a K-Nearest Neighbors (KNN) classifier or regressor is crucial for achieving \n",
    "good performance. Selecting the right value for K depends on the characteristics of the data, and various techniques can be employed\n",
    "to determine the optimal K value. Here are some commonly used methods:\n",
    "\n",
    "Grid Search with Cross-Validation:\n",
    "\n",
    "Perform a grid search over a range of K values and use cross-validation to evaluate the performance of the model for each K.\n",
    "Choose the K that results in the best cross-validated performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaea2ec-c79c-4fcb-9697-fd53254f4068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "\n",
    "# Example for classification\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "param_grid = {'n_neighbors': [1, 3, 5, 7, 9]}\n",
    "grid_search = GridSearchCV(knn_classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "optimal_k_classifier = grid_search.best_params_['n_neighbors']\n",
    "\n",
    "# Example for regression\n",
    "knn_regressor = KNeighborsRegressor()\n",
    "param_grid = {'n_neighbors': [1, 3, 5, 7, 9]}\n",
    "grid_search = GridSearchCV(knn_regressor, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "optimal_k_regressor = grid_search.best_params_['n_neighbors']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72feb5e4-9c4c-45dd-99ff-b537199ca569",
   "metadata": {},
   "outputs": [],
   "source": [
    "Elbow Method:\n",
    "\n",
    "Plot the model performance (e.g., accuracy or mean squared error) for different K values and look for an \"elbow\" point where the \n",
    "performance stabilizes.\n",
    "The K value at the elbow is often considered the optimal choice.\n",
    "python\n",
    "Copy code\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example for classification\n",
    "error_rate = []\n",
    "for k in range(1, 21):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    error_rate.append(np.mean(y_pred != y_test))\n",
    "\n",
    "plt.plot(range(1, 21), error_rate, marker='o')\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.show()\n",
    "Leave-One-Out Cross-Validation (LOOCV):\n",
    "\n",
    "Use LOOCV, a special case of cross-validation where each data point is used as a test set once.\n",
    "Evaluate the model for different K values and choose the K that results in the lowest error.\n",
    "python\n",
    "Copy code\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Example for classification\n",
    "error_rates = []\n",
    "for k in range(1, 21):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    error = 0\n",
    "    for train_index, test_index in loo.split(X):\n",
    "        X_train_loo, X_test_loo = X[train_index], X[test_index]\n",
    "        y_train_loo, y_test_loo = y[train_index], y[test_index]\n",
    "        knn.fit(X_train_loo, y_train_loo)\n",
    "        y_pred_loo = knn.predict(X_test_loo)\n",
    "        error += int(y_pred_loo != y_test_loo)\n",
    "    error_rates.append(error / len(X))\n",
    "\n",
    "plt.plot(range(1, 21), error_rates, marker='o')\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.title('LOOCV for Optimal K')\n",
    "plt.show()\n",
    "Choose the method that best fits your specific problem and dataset. It's important to balance model complexity and performance \n",
    "when selecting the optimal K value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915b2b7b-0c80-4bff-af42-0ccf34fa753d",
   "metadata": {},
   "source": [
    "## Qestion 3 --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3b99c4-dc68-46a5-bfa3-b030c78b7430",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "The choice of distance metric in K-Nearest Neighbors (KNN) can significantly impact the performance of a classifier or regressor.\n",
    "Different distance metrics measure the similarity or dissimilarity between data points in distinct ways.\n",
    "Two common distance metrics used in KNN are Euclidean distance and Manhattan distance. \n",
    "Here's how the choice of distance metric can affect performance and some considerations for choosing one over the other:\n",
    "\n",
    "Euclidean Distance:\n",
    "L2 norm or straight-line distance.\n",
    "Sensitive to differences along all dimensions.\n",
    "Works well when features have similar scales.\n",
    "May be influenced by irrelevant dimensions in high-dimensional spaces.\n",
    "Manhattan Distance:\n",
    "L1 norm or city block distance.\n",
    "Less sensitive to differences along individual dimensions.\n",
    "More robust when features have different scales.\n",
    "Suitable for data with a grid-like distribution.\n",
    "Considerations for Choosing a Distance Metric:\n",
    "Feature Scaling:\n",
    "\n",
    "Euclidean distance is sensitive to feature scales, so if features have significantly different scales, it might be necessary to use feature \n",
    "scaling. Manhattan distance is less affected by scale differences.\n",
    "Data Distribution:\n",
    "\n",
    "Consider the distribution of your data. If the data is spread out in a more isotropic manner, Euclidean distance may perform well. \n",
    "If the data is distributed in a grid-like fashion, Manhattan distance might be more suitable.\n",
    "Dimensionality:\n",
    "\n",
    "In high-dimensional spaces, the curse of dimensionality can affect Euclidean distance more than Manhattan distance. Manhattan distance\n",
    "may be a better choice in high-dimensional scenarios.\n",
    "Data Characteristics:\n",
    "\n",
    "The nature of your data and the specific characteristics of your problem should guide the choice. Experimentation with both metrics is\n",
    "often necessary to determine which performs better for a given application.\n",
    "Domain Knowledge:\n",
    "\n",
    "Consider any domain-specific knowledge or insights you may have about the problem. Some problems may naturally align with one distance \n",
    "metric over the other.\n",
    "Experimentation:\n",
    "\n",
    "It's common to experiment with both distance metrics during model development. Use cross-validation or other evaluation techniques to \n",
    "compare the performance of the classifier or regressor using different distance metrics.\n",
    "In summary, the choice between Euclidean and Manhattan distance should be made based on the characteristics of the data, the scales of\n",
    "features, the distribution of data, and the dimensionality of the feature space. There is no one-size-fits-all answer, and the best distance \n",
    "metric for a specific problem may require experimentation and careful consideration of these factors.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca079e1-f5b1-4cab-aeac-a74fd150fcb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bbdd3d-553b-41cb-b7e8-6d23c78b478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c26df2e-9a3a-4623-a65a-ab1cc285816f",
   "metadata": {},
   "source": [
    "## Qestion 4 --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a6a2d8-6c7f-465f-8267-855152d00d7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 1) (69029413.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    The performance of a K-Nearest Neighbors (KNN) model can be assessed using appropriate evaluation metrics based on the specific task, whether it's classification or regression. Here are common metrics for each task:\u001b[0m\n\u001b[0m                                                                                                                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1)\n"
     ]
    }
   ],
   "source": [
    "K-Nearest Neighbors (KNN) classifiers and regressors have hyperparameters that can significantly influence the performance of the model.\n",
    "Here are some common hyperparameters and their impact on model performance:\n",
    "\n",
    "Common Hyperparameters:\n",
    "Number of Neighbors (K):\n",
    "\n",
    "Effect: Determines the number of neighbors considered when making predictions. Smaller K values make the model more sensitive to noise, \n",
    "while larger K values may lead to smoothing and underfitting.\n",
    "Tuning: Perform a grid search or use cross-validation to find the optimal value for K.\n",
    "Distance Metric:\n",
    "\n",
    "Effect: Defines the method used to calculate distances between data points (e.g., Euclidean, Manhattan).\n",
    "The choice of distance metric affects the sensitivity of the model to feature scales and dimensions.\n",
    "Tuning: Experiment with different distance metrics based on data characteristics. Grid search or cross-validation can help find the best\n",
    "metric.\n",
    "Weighting of Neighbors:\n",
    "\n",
    "Effect: Specifies whether all neighbors have equal influence on predictions or if closer neighbors have more influence. Options \n",
    "include uniform weighting and distance-weighted (inverse distance) weighting.\n",
    "Tuning: Choose the appropriate weighting scheme based on the characteristics of the data. Grid search can help find the best weighting\n",
    "approach.\n",
    "Algorithm (Ball Tree, KD Tree, Brute Force):\n",
    "\n",
    "Effect: Determines the algorithm used to organize and search for neighbors. Different algorithms may have varying computational efficiency \n",
    "based on the dataset size and dimensionality.\n",
    "Tuning: Depending on the dataset size and dimensionality, choose the most suitable algorithm. The default (auto) often works well.\n",
    "Tuning Hyperparameters:\n",
    "Grid Search:\n",
    "\n",
    "Perform a grid search over a range of hyperparameter values and use cross-validation to evaluate model performance for each combination.\n",
    "Example for K value and distance metric in a KNN classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b126abe-75b5-437b-845b-1ce1d5b9b443",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "param_grid = {'n_neighbors': [1, 3, 5, 7, 9], 'metric': ['euclidean', 'manhattan']}\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn_classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "optimal_params = grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e32d7-449b-47e1-ba32-19c472ffe339",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random Search:\n",
    "\n",
    "Similar to grid search but samples hyperparameter values randomly from specified distributions. May be more efficient for large hyperparameter spaces.\n",
    "Cross-Validation:\n",
    "\n",
    "Use cross-validation to get a more robust estimate of model performance. It helps prevent overfitting to a specific train-test split and provides insights into generalization.\n",
    "Domain Knowledge:\n",
    "\n",
    "Leverage domain knowledge and insights about the problem to guide the selection of hyperparameter values. For example, \n",
    "the choice of K may be influenced by the characteristics of the dataset.\n",
    "Evaluate on a Validation Set:\n",
    "\n",
    "Split the data into training, validation, and test sets. Tune hyperparameters on the training set, validate on the validation set, \n",
    "and assess final performance on the test set.\n",
    "Remember that the optimal hyperparameter values depend on the specific characteristics of the data, so it's crucial to experiment with \n",
    "different configurations and evaluate their impact on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1527da51-4aff-4782-be83-21011f91a7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8855414a-fc38-4752-a881-fecee3c42499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fbae123-e8b6-4a9e-8691-16b297790f55",
   "metadata": {},
   "source": [
    "## Qestion 5 --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23803f32-dcb5-4597-bdc9-8b38f28d0325",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 41) (3637594134.py, line 41)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[33], line 41\u001b[0;36m\u001b[0m\n\u001b[0;31m    Let's illustrate the differences using a simple example with synthetic data:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 41)\n"
     ]
    }
   ],
   "source": [
    "The size of the training set can significantly impact the performance of a K-Nearest Neighbors (KNN) classifier or regressor. \n",
    "The relationship between the training set size and performance can be influenced by various factors, and optimizing the size of the training \n",
    "set is essential for achieving good generalization. Here are considerations regarding the impact of training set size and techniques\n",
    "for optimization:\n",
    "\n",
    "Impact of Training Set Size:\n",
    "Small Training Sets:\n",
    "\n",
    "In general, small training sets may lead to overfitting, especially if the dataset has complex patterns that require a sufficient\n",
    "amount of data to generalize well.\n",
    "The model may become sensitive to noise and exhibit poor performance on new, unseen data.\n",
    "Large Training Sets:\n",
    "\n",
    "Larger training sets often provide more representative samples of the underlying data distribution, helping the model to capture general \n",
    "trends and patterns.\n",
    "However, increasing the training set size indefinitely does not guarantee continuous improvement and may result in diminishing returns.\n",
    "Techniques to Optimize Training Set Size:\n",
    "Cross-Validation:\n",
    "\n",
    "Use cross-validation to assess model performance across different training set sizes. Cross-validation helps estimate how well the model \n",
    "generalizes to unseen data and can guide the choice of an optimal training set size.\n",
    "Learning Curves:\n",
    "\n",
    "Plot learning curves that show the model's performance on the training and validation sets as a function of the training set size.\n",
    "This visual representation can help identify points of diminishing returns or overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd66d044-7bd5-4715-9bd5-0488760218bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_sizes, train_scores, val_scores = learning_curve(model, X, y, train_sizes=[0.1, 0.2, 0.5, 0.8, 1.0], cv=5)\n",
    "\n",
    "# Plot learning curves\n",
    "plt.plot(train_sizes, np.mean(train_scores, axis=1), label='Training Score')\n",
    "plt.plot(train_sizes, np.mean(val_scores, axis=1), label='Validation Score')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Performance Score')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79f1084-734f-4c2e-b59d-cfe7be5d23ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Incremental Learning:\n",
    "\n",
    "Implement incremental or online learning strategies where the model is updated as new data becomes available. This is useful for scenarios \n",
    "where the training set evolves over time.\n",
    "Stratified Sampling:\n",
    "\n",
    "If the dataset is imbalanced, ensure that the training set includes representative samples from each class. Stratified sampling can help\n",
    "maintain class balance and improve model performance.\n",
    "Data Augmentation:\n",
    "\n",
    "For certain tasks, such as image classification, data augmentation techniques can be used to artificially increase the effective size of the\n",
    "training set by applying transformations (e.g., rotations, flips) to existing samples.\n",
    "Feature Selection or Dimensionality Reduction:\n",
    "\n",
    "In cases of high-dimensional data, reducing the number of features can make the model more efficient and reduce the risk of overfitting, \n",
    "especially when the training set size is limited.\n",
    "Optimizing the training set size involves a trade-off between having sufficient data for the model to generalize and the computational \n",
    "resources required. By carefully monitoring performance metrics, learning curves, and considering the specific characteristics of the problem,\n",
    "practitioners can make informed decisions about the appropriate size of the training set for a KNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ddcde6-8491-4209-b16b-37c212415cb5",
   "metadata": {},
   "source": [
    "## Qestion 6 --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ddcb44-770f-4b20-a2b0-c589babfffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "While K-Nearest Neighbors (KNN) is a simple and intuitive algorithm, it comes with certain drawbacks that can affect its performance \n",
    "in certain scenarios. Understanding these drawbacks is important for effective model selection and improvement. Here are some potential \n",
    "drawbacks of using KNN as a classifier or regressor and strategies to overcome them:\n",
    "\n",
    "Potential Drawbacks:\n",
    "Computational Complexity:\n",
    "\n",
    "KNN can be computationally expensive, especially when dealing with large datasets or high-dimensional feature spaces. Calculating \n",
    "distances between data points becomes more time-consuming as the dataset size increases.\n",
    "Mitigation:\n",
    "\n",
    "Consider using approximate nearest neighbors algorithms or dimensionality reduction techniques to reduce computational complexity.\n",
    "Implement efficient data structures like Ball Trees or KD Trees for nearest neighbor search.\n",
    "Sensitivity to Noise and Outliers:\n",
    "\n",
    "KNN is sensitive to noisy data and outliers, as they can significantly impact the calculation of distances and influence predictions.\n",
    "Mitigation:\n",
    "\n",
    "Outlier detection and removal techniques can be applied before applying KNN.\n",
    "Consider using distance-weighted KNN, where closer neighbors have more influence on predictions.\n",
    "Curse of Dimensionality:\n",
    "\n",
    "As the number of dimensions increases, the distance between data points also increases, leading to sparsity and challenges in \n",
    "finding meaningful neighbors.\n",
    "Mitigation:\n",
    "\n",
    "Use dimensionality reduction techniques such as Principal Component Analysis (PCA) or feature selection to reduce the number of dimensions.\n",
    "Experiment with different distance metrics that might be less affected by the curse of dimensionality.\n",
    "Need for Feature Scaling:\n",
    "\n",
    "Features with larger scales can dominate the distance calculations, leading to biased results.\n",
    "Mitigation:\n",
    "\n",
    "Apply feature scaling techniques such as Min-Max scaling or Standardization to ensure that all features contribute equally to\n",
    "distance calculations.\n",
    "Optimal K Selection:\n",
    "\n",
    "The choice of the hyperparameter K is critical, and selecting an inappropriate value can lead to overfitting or underfitting.\n",
    "Mitigation:\n",
    "\n",
    "Use cross-validation, grid search, or random search to find the optimal K value.\n",
    "Implement techniques like the Elbow Method to identify a suitable K based on performance metrics.\n",
    "Imbalanced Data:\n",
    "\n",
    "KNN can be affected by imbalanced class distributions, especially in classification tasks.\n",
    "Mitigation:\n",
    "\n",
    "Consider using techniques such as oversampling, undersampling, or generating synthetic samples to balance the class distribution.\n",
    "Utilize stratified sampling during cross-validation to maintain class balance.\n",
    "Memory Usage:\n",
    "\n",
    "Storing the entire training dataset in memory can be impractical for large datasets.\n",
    "Mitigation:\n",
    "\n",
    "Use algorithms that support efficient indexing or partial fitting for large datasets.\n",
    "Employ approximate nearest neighbor search methods to reduce memory requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
